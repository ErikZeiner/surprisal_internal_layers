/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/torch/cuda/__init__.py:829: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
Traceback (most recent call last):
  File "/gpfs/bwfor/home/tu/tu_tu/tu_zxolr37/surprisal_internal_layers/src/EZ_run_gpt2_transformer_lens.py", line 15, in <module>
    from transformer_lens import HookedTransformer
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/transformer_lens/__init__.py", line 8, in <module>
    from . import components
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/transformer_lens/components/__init__.py", line 9, in <module>
    from .abstract_attention import AbstractAttention
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/transformer_lens/components/abstract_attention.py", line 22, in <module>
    import bitsandbytes as bnb
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/bitsandbytes/__init__.py", line 15, in <module>
    from .nn import modules
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/bitsandbytes/nn/__init__.py", line 21, in <module>
    from .triton_based_modules import (
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/bitsandbytes/nn/triton_based_modules.py", line 7, in <module>
    from bitsandbytes.triton.int8_matmul_mixed_dequantize import (
  File "/home/tu/tu_tu/tu_zxolr37/.conda/envs/surprisal/lib/python3.11/site-packages/bitsandbytes/triton/int8_matmul_mixed_dequantize.py", line 12, in <module>
    from triton.ops.matmul_perf_model import early_config_prune, estimate_matmul_time
ModuleNotFoundError: No module named 'triton.ops'
gpt2
1
  0%|          | 0/75 [00:00<?, ?it/s]  1%|▏         | 1/75 [00:04<05:30,  4.47s/it]  3%|▎         | 2/75 [00:10<06:34,  5.41s/it]  4%|▍         | 3/75 [00:16<06:55,  5.78s/it]  5%|▌         | 4/75 [00:23<07:27,  6.30s/it]  7%|▋         | 5/75 [00:26<05:56,  5.09s/it]  8%|▊         | 6/75 [00:31<05:45,  5.00s/it]  9%|▉         | 7/75 [00:39<06:38,  5.86s/it] 11%|█         | 8/75 [00:46<07:09,  6.41s/it] 12%|█▏        | 9/75 [00:49<05:40,  5.16s/it] 13%|█▎        | 10/75 [00:52<05:03,  4.66s/it] 15%|█▍        | 11/75 [00:56<04:41,  4.40s/it] 16%|█▌        | 12/75 [01:00<04:26,  4.22s/it] 17%|█▋        | 13/75 [01:03<04:05,  3.96s/it] 19%|█▊        | 14/75 [01:07<04:03,  3.99s/it] 20%|██        | 15/75 [01:16<05:23,  5.39s/it] 21%|██▏       | 16/75 [01:22<05:29,  5.59s/it] 23%|██▎       | 17/75 [01:28<05:35,  5.79s/it] 24%|██▍       | 18/75 [01:36<06:02,  6.36s/it] 25%|██▌       | 19/75 [01:39<05:05,  5.46s/it] 27%|██▋       | 20/75 [01:42<04:12,  4.59s/it] 28%|██▊       | 21/75 [01:46<03:53,  4.33s/it] 29%|██▉       | 22/75 [01:50<03:42,  4.21s/it] 31%|███       | 23/75 [01:53<03:21,  3.88s/it] 32%|███▏      | 24/75 [01:55<02:56,  3.46s/it] 33%|███▎      | 25/75 [01:59<02:52,  3.46s/it] 35%|███▍      | 26/75 [02:02<02:43,  3.33s/it] 36%|███▌      | 27/75 [02:05<02:36,  3.26s/it] 37%|███▋      | 28/75 [02:07<02:23,  3.06s/it] 39%|███▊      | 29/75 [02:14<03:08,  4.09s/it] 40%|████      | 30/75 [02:18<02:59,  3.99s/it] 41%|████▏     | 31/75 [02:21<02:52,  3.91s/it] 43%|████▎     | 32/75 [02:26<02:54,  4.05s/it] 44%|████▍     | 33/75 [02:29<02:41,  3.85s/it] 45%|████▌     | 34/75 [02:33<02:36,  3.82s/it] 47%|████▋     | 35/75 [02:37<02:37,  3.94s/it] 48%|████▊     | 36/75 [02:41<02:28,  3.81s/it] 49%|████▉     | 37/75 [02:44<02:18,  3.65s/it] 51%|█████     | 38/75 [02:48<02:25,  3.92s/it] 52%|█████▏    | 39/75 [02:52<02:20,  3.90s/it] 53%|█████▎    | 40/75 [02:56<02:14,  3.86s/it] 55%|█████▍    | 41/75 [03:01<02:24,  4.26s/it] 56%|█████▌    | 42/75 [03:05<02:12,  4.03s/it] 57%|█████▋    | 43/75 [03:08<01:58,  3.71s/it] 59%|█████▊    | 44/75 [03:12<02:02,  3.96s/it] 60%|██████    | 45/75 [03:16<02:00,  4.01s/it] 61%|██████▏   | 46/75 [03:20<01:51,  3.85s/it] 63%|██████▎   | 47/75 [03:24<01:51,  3.99s/it] 64%|██████▍   | 48/75 [03:27<01:38,  3.65s/it] 65%|██████▌   | 49/75 [03:29<01:26,  3.31s/it] 67%|██████▋   | 50/75 [03:36<01:49,  4.40s/it] 68%|██████▊   | 51/75 [03:41<01:50,  4.59s/it] 69%|██████▉   | 52/75 [03:44<01:31,  4.00s/it] 71%|███████   | 53/75 [03:47<01:22,  3.75s/it] 72%|███████▏  | 54/75 [03:52<01:24,  4.00s/it] 73%|███████▎  | 55/75 [03:57<01:24,  4.25s/it] 75%|███████▍  | 56/75 [04:00<01:15,  3.97s/it] 76%|███████▌  | 57/75 [04:03<01:06,  3.69s/it] 77%|███████▋  | 58/75 [04:10<01:17,  4.58s/it] 79%|███████▊  | 59/75 [04:12<01:03,  3.94s/it] 80%|████████  | 60/75 [04:18<01:07,  4.53s/it] 81%|████████▏ | 61/75 [04:21<00:57,  4.14s/it] 83%|████████▎ | 62/75 [04:25<00:50,  3.88s/it] 84%|████████▍ | 63/75 [04:31<00:54,  4.54s/it] 85%|████████▌ | 64/75 [04:37<00:54,  4.98s/it] 87%|████████▋ | 65/75 [04:40<00:43,  4.38s/it] 88%|████████▊ | 66/75 [04:42<00:33,  3.75s/it] 89%|████████▉ | 67/75 [04:45<00:28,  3.59s/it] 91%|█████████ | 68/75 [04:50<00:28,  4.07s/it] 92%|█████████▏| 69/75 [04:55<00:25,  4.29s/it] 93%|█████████▎| 70/75 [05:00<00:22,  4.57s/it] 95%|█████████▍| 71/75 [05:04<00:17,  4.31s/it] 96%|█████████▌| 72/75 [05:06<00:10,  3.59s/it] 97%|█████████▋| 73/75 [05:10<00:07,  3.79s/it] 99%|█████████▊| 74/75 [05:14<00:03,  3.66s/it]100%|██████████| 75/75 [05:20<00:00,  4.64s/it]100%|██████████| 75/75 [05:20<00:00,  4.28s/it]
SUCCESSFUL RUN: gpt2 ZuCO
